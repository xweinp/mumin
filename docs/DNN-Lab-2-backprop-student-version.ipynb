{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xweinp/mumin/blob/main/docs/DNN-Lab-2-backprop-student-version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxaMTckGDGgc"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFrkgOd8e42J"
      },
      "source": [
        "# Laboratory Scenario 2 - Backpropagation and Gradient Checkpointing\n",
        "\n",
        "In this lab scenario, you are given an implementation of a simple neural network, and your goal is to implement the backpropagation procedure for this network.  \n",
        "\n",
        "To be more precise, the network inputs a tensor $x$ of shape `(MINI_BATCH_SIZE, 28*28)`, where each element of the batch represents a flattened grayscale image of shape `(28, 28)`.  \n",
        "In exercise 1, you can assume that images in the minibatch are fed to the network one by one (as tensors of shape `(1, 28*28)` - single image and `(1, 10)` - image class).  \n",
        "In exercise 2 you are asked to make the backpropagation work without this assumption, on whole mini-batches.  \n",
        "In exercise 3, you will implement a technique called *gradient checkpointing*, that allows you to reduce the amount of memory used to store activations for backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y4l5BmxTNNU"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import Sequence, Iterator, cast\n",
        "from typing_extensions import override\n",
        "\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from numpy.typing import NDArray\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "FloatNDArray = NDArray[np.float64]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4UpigUXf4Xt"
      },
      "source": [
        "## Loading the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHhqeGLsHcYl"
      },
      "outputs": [],
      "source": [
        "!wget --no-verbose -O mnist.npz https://s3.amazonaws.com/img-datasets/mnist.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uutaqUkuVAuF"
      },
      "outputs": [],
      "source": [
        "def load_mnist(\n",
        "    path: Path = Path('mnist.npz')\n",
        ") -> tuple[FloatNDArray, FloatNDArray, FloatNDArray, FloatNDArray]:\n",
        "    '''\n",
        "    Load the MNIST dataset (grayscale 28 x 28 images of hand-written digits).\n",
        "\n",
        "    Returns tuple of:\n",
        "    - x_train: shape (N_train, H * W), grayscale values 0..1.\n",
        "    - y_train: shape (N_train, 10), one-hot-encoded label, dtype float64.\n",
        "    - x_test: shape (N_test, H * W), grayscale values 0..1.\n",
        "    - y_train: shape (N_test, 10), one-hot-encoded label, dtype float64.\n",
        "\n",
        "    More: https://en.wikipedia.org/wiki/MNIST_database\n",
        "    '''\n",
        "    with np.load(path) as f:\n",
        "        x_train, _y_train = f['x_train'], f['y_train']\n",
        "        x_test, _y_test = f['x_test'], f['y_test']\n",
        "\n",
        "    H = W = 28\n",
        "    N_train = len(x_train)\n",
        "    N_test = len(x_test)\n",
        "    assert x_train.shape == (N_train, H, W) and _y_train.shape == (N_train,)\n",
        "    assert x_test.shape == (N_test, H, W) and _y_test.shape == (N_test,)\n",
        "\n",
        "    x_train = x_train.reshape(N_train, H * W) / 255.0\n",
        "    x_test = x_test.reshape(N_test, H * W) / 255.0\n",
        "\n",
        "    y_train = np.zeros((N_train, 10), dtype=np.float64)\n",
        "    y_train[np.arange(N_train), _y_train] = 1\n",
        "\n",
        "    y_test = np.zeros((N_test, 10))\n",
        "    y_test[np.arange(N_test), _y_test] = 1\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_mnist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-icrjrxmizjf"
      },
      "outputs": [],
      "source": [
        "def to_pillow_image(x: FloatNDArray, scale: int = 10) -> PIL.Image.Image:\n",
        "    '''Convert example of shape (28 * 28,) and values 0..1 to Pillow image.'''\n",
        "    H = W = 28\n",
        "    assert x.shape == (H * W,) and 0 <= x.min() <= x.max() <= 1\n",
        "    x_int = (x * 255).astype(np.uint8).reshape(H, W)\n",
        "    img = PIL.Image.fromarray(x_int, mode='L')\n",
        "    return img.resize((H * scale, W * scale), PIL.Image.Resampling.NEAREST)\n",
        "\n",
        "\n",
        "display(to_pillow_image(x_train[0]))\n",
        "print(y_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5PPE1ldTNNx"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "In this exercise, your task is to fill in the gaps in this code by implementing the backpropagation algorithm.\n",
        "Once done, you can run the network on the MNIST example and see how it performs.  \n",
        "Feel free to play with the parameters. Your model should achieve 90%+ accuracy after a few epochs.  \n",
        "\n",
        "Before you start you should note a few things:\n",
        "+ `backprop` - is the function that you need to implement\n",
        "+ `learning_step` - calls `backprop` to get the gradients for network parameters\n",
        "+ The derivative of the loss is already computed by `cost_derivative`.\n",
        "+ Your goal is to compute $\\frac{d L\\left(\\text{model}(x), y\\right)}{d p}$ for each parameter $p$ of the network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvbfIFaF5_MW"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z: FloatNDArray) -> FloatNDArray:\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z: FloatNDArray) -> FloatNDArray:\n",
        "    '''Derivative of the sigmoid.'''\n",
        "    return sigmoid(z) * (1 - sigmoid(z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsCgwvfHTNN0"
      },
      "outputs": [],
      "source": [
        "class Network:\n",
        "    '''\n",
        "    Multi-Layer Perceptron.\n",
        "\n",
        "    A simple neural network with fully-connected layers and sigmoid activations.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, sizes: Sequence[int] = (784, 30, 10)):\n",
        "        '''\n",
        "        Args:\n",
        "        - sizes: sequence of layer widths [N^0, ... , N^last]\n",
        "          These are lengths of activation vectors, where:\n",
        "          - N^0 is input size: 784.\n",
        "          - N^last is the number of classes into which we can classify each input: 10.\n",
        "        '''\n",
        "        self.sizes = list(sizes)\n",
        "\n",
        "        # We initialize weights and biases with random normal distribution.\n",
        "\n",
        "        # List of len(sizes) - 1 vectors of shape (N^1), (N^2), ..., (N^last).\n",
        "        self.biases = [np.random.randn(n) for n in sizes[1:]]\n",
        "\n",
        "        # List of len(sizes) - 1 matrices of shape (N^i, N^{i-1}).\n",
        "        # Weights are indexed by target node first.\n",
        "        self.weights = [\n",
        "            np.random.randn(n_out, n_in) / np.sqrt(n_in)\n",
        "            for n_in, n_out in zip(sizes[:-1], sizes[1:], strict=True)\n",
        "        ]\n",
        "\n",
        "    def feedforward(self, x: FloatNDArray) -> FloatNDArray:\n",
        "        '''\n",
        "        Run the network on a single case of shape (N^0,).\n",
        "\n",
        "        Returns last layer activations, shape (N^last,), values 0..1.\n",
        "        '''\n",
        "        g = x\n",
        "        for w, b in zip(self.weights, self.biases, strict=True):\n",
        "            # TODO\n",
        "            # f = # pre-activations\n",
        "            # g = # activations\n",
        "            pass\n",
        "        return g\n",
        "\n",
        "    def learning_step(\n",
        "        self, x_mini_batch: FloatNDArray, y_mini_batch: FloatNDArray, learning_rate: float\n",
        "    ) -> None:\n",
        "        '''\n",
        "        Update network parameters with a single mini-batch step of backpropagation and gradient descent.\n",
        "\n",
        "        Args:\n",
        "        - x_mini_batch: shape (B, N^0) where B is the mini-batch size.\n",
        "        - y_mini_batch: shape (B, N^last).\n",
        "        - learning_rate.\n",
        "        '''\n",
        "        # Accumulate gradients by running backprop one dataitem at a time (without vectorization).\n",
        "        grads_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        grads_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        for x, y in zip(x_mini_batch, y_mini_batch):\n",
        "            item_grads_w, item_grads_b = self.backprop(x, y)\n",
        "            for i in range(len(grads_w)):\n",
        "                grads_w[i] = grads_w[i] + item_grads_w[i]\n",
        "            for i in range(len(grads_b)):\n",
        "                grads_b[i] = grads_b[i] + item_grads_b[i]\n",
        "\n",
        "        # Gradient descent step.\n",
        "        self.weights = [\n",
        "            w - grad_w * (learning_rate / len(x_mini_batch))\n",
        "            for w, grad_w in zip(self.weights, grads_w, strict=True)\n",
        "        ]\n",
        "        self.biases = [\n",
        "            b - grad_b * (learning_rate / len(x_mini_batch))\n",
        "            for b, grad_b in zip(self.biases, grads_b, strict=True)\n",
        "        ]\n",
        "\n",
        "    def backprop(\n",
        "        self, x: FloatNDArray, y: FloatNDArray\n",
        "    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n",
        "        '''\n",
        "        Backpropagation for a single input (not vectorized).\n",
        "\n",
        "        Args:\n",
        "        - x: input features, shape (N^0).\n",
        "        - y: target label (one-hot encoded), shape (N^last).\n",
        "\n",
        "        Returns (grads_w, grads_b), where:\n",
        "        - grads_w is a list of gradients over weights (shape (N^i, N^{i-1})), for each layer.\n",
        "        - grads_b is a list of gradients over biases (shape (N^i)), for each layer.\n",
        "        '''\n",
        "\n",
        "        # Go forward, remembering all activations.\n",
        "        # Pre-activations function, layer by layer, shapes (N^1), ..., (N^last).\n",
        "        fs: list[FloatNDArray] = []\n",
        "        # Activations (including inputs to the first layer), shapes (N^0), (N^1), ..., (N^last).\n",
        "        gs: list[FloatNDArray] = [x]\n",
        "\n",
        "        # TODO forward pass.\n",
        "\n",
        "        assert [f.shape for f in fs] == [(n,) for n in self.sizes[1:]], f'Shape mismatch: {[f.shape for f in fs]} vs {self.sizes[1:]}'\n",
        "        assert [g.shape for g in gs] == [(n,) for n in self.sizes], f'Shape mismatch: {[g.shape for g in gs]} vs {self.sizes}'\n",
        "\n",
        "        # Now go backward from the final cost applying backpropagation.\n",
        "        grad_g = self.cost_derivative(gs[-1], y)  # shape initially (N^last), then layer by layer.\n",
        "\n",
        "        # TODO backward pass. May be useful:\n",
        "        # - reversed(list(...)) can be used to iterate in reverse.\n",
        "        # - list.reverse() reverses a list in-place.\n",
        "        # - np.outer() computes an outer product (a_{i,j} = b_i c_j) very fast.\n",
        "\n",
        "        grads_w = []\n",
        "        grads_b = []\n",
        "\n",
        "        # Now grads_w should have shapes (N^1, N^0), ..., (N^last, N^{last-1}).\n",
        "        # Now grads_b should have shapes (N^1), ..., (N^last).\n",
        "        for grad_b, b in zip(grads_b, self.biases, strict=True):\n",
        "            assert grad_b.shape == b.shape, f'Shape mismatch: {grad_b.shape=} but {b.shape=}'\n",
        "        for grad_w, w in zip(grads_w, self.weights, strict=True):\n",
        "            assert grad_w.shape == w.shape, f'Shape mismatch: {grad_w.shape=} but {w.shape=}'\n",
        "\n",
        "        return grads_w, grads_b\n",
        "\n",
        "    def cost_derivative(self, g: FloatNDArray, y: FloatNDArray) -> FloatNDArray:\n",
        "        '''\n",
        "        Gradient of loss (MSE) over output activations, for a single sample.\n",
        "\n",
        "        Args:\n",
        "        - g: output activations, shape (N^last).\n",
        "        - y: target values (one-hot encoded labels), shape (N^last).\n",
        "\n",
        "        Returns gradients, shape (N^last).\n",
        "        '''\n",
        "        assert g.shape == y.shape, f'Shape mismatch: {g.shape=} but {y.shape=}'\n",
        "        N_last, = a.shape\n",
        "        return (2 / N_last) * (a - y.astype(np.float64))\n",
        "\n",
        "    def evaluate(\n",
        "        self, x_test_data: FloatNDArray, y_test_data: FloatNDArray\n",
        "    ) -> np.float64:\n",
        "        '''\n",
        "        Compute accuracy: the ratio of correct answers for test_data.\n",
        "\n",
        "        Args (here B is the number of test dataitems):\n",
        "        - x_test_data: shape (B, N^0).\n",
        "        - y_test_data: shape (B, N^last).\n",
        "        '''\n",
        "        test_results: list[bool] = []\n",
        "        for x, y in zip(x_test_data, y_test_data):\n",
        "            output_label: np.int64 = np.argmax(self.feedforward(x))\n",
        "            target_label: np.int64 = np.argmax(y)\n",
        "            test_results.append(output_label == target_label)\n",
        "\n",
        "        return np.mean(test_results)\n",
        "\n",
        "    def SGD(\n",
        "        self,\n",
        "        training_data: tuple[FloatNDArray, FloatNDArray],\n",
        "        test_data: tuple[FloatNDArray, FloatNDArray] | None = None,\n",
        "        epochs: int = 2,\n",
        "        mini_batch_size: int = 100,\n",
        "        learning_rate: float = 1.0,\n",
        "    ) -> None:\n",
        "        x_train, y_train = training_data\n",
        "        for epoch in tqdm(range(epochs)):\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                i_begin = i * mini_batch_size\n",
        "                i_end = (i + 1) * mini_batch_size\n",
        "                self.learning_step(x_train[i_begin:i_end], y_train[i_begin:i_end], learning_rate)\n",
        "            if test_data:\n",
        "                x_test, y_test = test_data\n",
        "                accuracy = self.evaluate(x_test, y_test)\n",
        "                tqdm.write(f'Epoch: {epoch}, Accuracy: {accuracy * 100:.2f} %')\n",
        "            else:\n",
        "                tqdm.write(f'Epoch: {epoch}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CBP5a-30tRF"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Smaller test on part of the dataset.\n",
        "# The unvectorized version should take about 2s per epoch\n",
        "# and achieve accuracy ~75% or more (for most executions).\n",
        "network = Network([784, 30, 10])\n",
        "network.SGD(\n",
        "    (x_train[:3000], y_train[:3000]),\n",
        "    test_data=(x_test[:3000], y_test[:3000]),\n",
        "    epochs=5,\n",
        "    mini_batch_size=100,\n",
        "    learning_rate=10.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JSUVlz11rHl"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Full test.\n",
        "# The unvectorized version should take about 20s per epoch\n",
        "# and achieve accuracy ~90% or more.\n",
        "network = Network([784, 30, 10])\n",
        "network.SGD(\n",
        "    (x_train, y_train),\n",
        "    test_data=(x_test, y_test),\n",
        "    epochs=10,\n",
        "    mini_batch_size=100,\n",
        "    learning_rate=5.0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d88s6sr-50HK"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "Implement a \"fully vectorized\" version, i.e. one using matrix operations instead of going over examples one by one within a minibatch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01WQss4d5xC-"
      },
      "outputs": [],
      "source": [
        "class NetworkVectorized:\n",
        "    '''Multi-Layer Perceptron with vectorized (batched) methods.'''\n",
        "\n",
        "    def __init__(self, sizes: Sequence[int] = (784, 30, 10)):\n",
        "        '''\n",
        "        Args:\n",
        "        - sizes: sequence of layer widths [N^0, ... , N^last]\n",
        "          These are lengths of activation vectors, where:\n",
        "          - N^0 is input size: 784.\n",
        "          - N^last is the number of classes into which we can classify each input: 10.\n",
        "        '''\n",
        "        self.sizes = list(sizes)\n",
        "\n",
        "        # We initialize weights and biases with random normal distribution.\n",
        "\n",
        "        # List of len(sizes) - 1 vectors of shape (N^1), (N^2), ..., (N^last)\n",
        "        self.biases = [np.random.randn(n) for n in sizes[1:]]\n",
        "\n",
        "        # List of len(sizes) - 1 matrices of shape (N^i, N^{i-1}).\n",
        "        # Weights are indexed by target node first.\n",
        "        self.weights = [\n",
        "            np.random.randn(n_out, n_in) / np.sqrt(n_in)\n",
        "            for n_in, n_out in zip(sizes[:-1], sizes[1:], strict=True)\n",
        "        ]\n",
        "\n",
        "    def feedforward(self, x: FloatNDArray) -> FloatNDArray:\n",
        "        '''\n",
        "        Run the network on a batch of cases of shape (B, N^0), values 0..1.\n",
        "\n",
        "        Returns last layer activations, shape (B, N^last), values 0..1.\n",
        "        '''\n",
        "        g = x\n",
        "        for w, b in zip(self.weights, self.biases, strict=True):\n",
        "            # TODO\n",
        "            pass\n",
        "        return g\n",
        "\n",
        "    def learning_step(self, x_mini_batch: FloatNDArray, y_mini_batch: FloatNDArray, learning_rate: float) -> None:\n",
        "        '''\n",
        "        Update network parameters with a single mini-batch step of backpropagation and gradient descent.\n",
        "\n",
        "        Args:\n",
        "        - x_mini_batch: shape (B, N^0) where B is mini_batch_size.\n",
        "        - y_mini_batch: shape (B, N^last).\n",
        "        - learning_rate.\n",
        "        '''\n",
        "        grads_w, grads_b = self.backprop_vectorized(x_mini_batch, y_mini_batch)\n",
        "\n",
        "        # Gradient descent step.\n",
        "        self.weights = [\n",
        "            w - learning_rate * grad_w\n",
        "            for w, grad_w in zip(self.weights, grads_w, strict=True)\n",
        "        ]\n",
        "        self.biases = [\n",
        "            b - learning_rate * grad_b\n",
        "            for b, grad_b in zip(self.biases, grads_b, strict=True)\n",
        "        ]\n",
        "\n",
        "    def backprop_vectorized(\n",
        "        self, x: FloatNDArray, y: FloatNDArray\n",
        "    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n",
        "        '''Backpropagation for a mini-batch (vectorized).\n",
        "\n",
        "        Args:\n",
        "        - x: input, shape (B, N^0)\n",
        "        - y: target label (one-hot encoded), shape (B, N^last)\n",
        "\n",
        "        Returns (grads_w, grads_b), where:\n",
        "        - grads_w: list of gradients over weights (shape (N^i, N^{i-1})), for each layer.\n",
        "        - grads_b: list of gradients over biases (shape (N^i)), for each layer.\n",
        "        '''\n",
        "        B, N0 = x.shape\n",
        "        assert N0 == self.sizes[0]\n",
        "\n",
        "        # Go forward, remembering all activations.\n",
        "\n",
        "        # Values after activation function (including inputs to the first layer),\n",
        "        # shapes (B, N^0), (B, N^1), ..., (B, N^last).\n",
        "        gs: list[FloatNDArray] = [x]\n",
        "\n",
        "        # TODO\n",
        "\n",
        "        assert [g.shape for g in gs] == [(B, n) for n in self.sizes], \\\n",
        "            f'Shape mismatch: {[g.shape for g in gs]} vs {[(B, n) for n in self.sizes]}'\n",
        "\n",
        "        # Now go backward from the final cost applying backpropagation.\n",
        "        grad_g = self.cost_derivative(gs[-1], y)  # shape initially (B, N^last), then layer by layer.\n",
        "\n",
        "        # TODO\n",
        "        grads_w = []\n",
        "        grads_b = []\n",
        "\n",
        "        # Now grads_w should have shapes (N^1, N^0), ..., (N^last, N^{last-1}).\n",
        "        # Now grads_b should have shapes (N^1,) ..., (N^last,).\n",
        "        for grad_b, b in zip(grads_b, self.biases, strict=True):\n",
        "            assert grad_b.shape == b.shape, f'Shape mismatch: {grad_b.shape=} but {b.shape=}'\n",
        "        for grad_w, w in zip(grads_w, self.weights, strict=True):\n",
        "            assert grad_w.shape == w.shape, f'Shape mismatch: {grad_w.shape=} but {w.shape=}'\n",
        "\n",
        "        return grads_w, grads_b\n",
        "\n",
        "    def cost_derivative(self, g: FloatNDArray, y: FloatNDArray) -> FloatNDArray:\n",
        "        '''\n",
        "        Gradient of loss (MSE) over output activations.\n",
        "\n",
        "        Args:\n",
        "        - g: output activations, shape (B, N^last).\n",
        "        - y: target values (one-hot encoded labels), shape (B, N^last).\n",
        "\n",
        "        Returns gradients, shape (B, N^last).\n",
        "        '''\n",
        "        assert g.shape == y.shape, f'Shape mismatch: {g.shape=} but {y.shape=}'\n",
        "        B, N_last = a.shape\n",
        "        return (2 / (B * N_last)) * (a - y.astype(np.float64))\n",
        "\n",
        "    def evaluate(self, x_test_data: FloatNDArray, y_test_data: FloatNDArray) -> np.float64:\n",
        "        '''\n",
        "        Compute accuracy: the ratio of correct answers for test_data.\n",
        "\n",
        "        Args:\n",
        "        - x_test_data: shape (B, N^0).\n",
        "        - y_test_data: shape (B, N^last).\n",
        "        '''\n",
        "        predictions = np.argmax(self.feedforward(x_test_data), axis=1)\n",
        "        targets = np.argmax(y_test_data, axis=1)\n",
        "        return np.mean(predictions == targets)\n",
        "\n",
        "    def SGD(\n",
        "        self,\n",
        "        training_data: tuple[FloatNDArray, FloatNDArray],\n",
        "        test_data: tuple[FloatNDArray, FloatNDArray] | None = None,\n",
        "        epochs: int = 2,\n",
        "        mini_batch_size: int = 100,\n",
        "        learning_rate: float = 1.0\n",
        "    ) -> None:\n",
        "        x_train, y_train = training_data\n",
        "        for epoch in tqdm(range(epochs)):\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                i_begin = i * mini_batch_size\n",
        "                i_end = (i + 1) * mini_batch_size\n",
        "                self.learning_step(x_train[i_begin:i_end], y_train[i_begin:i_end], learning_rate)\n",
        "            if test_data:\n",
        "                x_test, y_test = test_data\n",
        "                accuracy = self.evaluate(x_test, y_test)\n",
        "                print(f'Epoch: {epoch}, Accuracy: {accuracy * 100:.2f} %')\n",
        "            else:\n",
        "                print(f'Epoch: {epoch}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjyZVxp59pxG"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# The vectorized version takes about ~1s per epoch.\n",
        "network = NetworkVectorized([784, 30, 10])\n",
        "network.SGD(\n",
        "    (x_train, y_train),\n",
        "    test_data=(x_test, y_test),\n",
        "    epochs=50,\n",
        "    mini_batch_size=100,\n",
        "    learning_rate=5.0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nIZiS_Ze42M"
      },
      "source": [
        "# Exercise 3 (optional)\n",
        "\n",
        "The standard backpropagation method requires memorization of all outputs of all layers computed during the forward pass, for use in the backward pass, which can take much of precious GPU memory.\n",
        "Instead of doing that, one can checkpoint (memorize) only activations of a select few layers and then recompute the rest as they are needed (redoing the forward pass between checkpoints).  \n",
        "Your task is to complete the code below to implement backpropagation with checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRdWr074ffey"
      },
      "outputs": [],
      "source": [
        "class NetworkWithCheckpoints(NetworkVectorized):\n",
        "    '''Multi-Layer Perceptron with gradient checkpointing.'''\n",
        "\n",
        "    def __init__(self, sizes: Sequence[int] = (784, 30, 10), checkpoints: Sequence[int] = (1,)):\n",
        "        '''\n",
        "        Args:\n",
        "        - sizes: sequence of layer widths [N^0, ... , N^last]\n",
        "          These are lengths of activation vectors, where:\n",
        "          - N^0 is input size: 784.\n",
        "          - N^last is the number of classes into which we can classify each input: 10.\n",
        "        - checkpoints: Indices of layers whose activations we want to checkpoint\n",
        "          (between 1 and last inclusive (last=len(sizes) - 1).\n",
        "        '''\n",
        "        super().__init__(sizes)\n",
        "        last_layer_id = len(sizes) - 1\n",
        "        # Always store the input and last activations.\n",
        "        self.checkpoints = sorted(set(checkpoints) | {0, last_layer_id})\n",
        "\n",
        "    def backprop_generator(\n",
        "        self, x: FloatNDArray, y: FloatNDArray\n",
        "    ) -> Iterator[tuple[FloatNDArray, FloatNDArray]]:\n",
        "        '''\n",
        "        Backpropagation for a mini-batch (vectorized).\n",
        "\n",
        "        Args:\n",
        "        - x: input, shape (B, N^0)\n",
        "        - y: target label (one-hot encoded), shape (B, N^last)\n",
        "\n",
        "        Yields (grad_w, grad_b) for each layer from i=last down to i=1, where:\n",
        "        - grads_w: shape (N^i, N^{i-1})).\n",
        "        - grads_b: shape (N^i)).\n",
        "        '''\n",
        "        B, N0 = x.shape\n",
        "        assert N0 == self.sizes[0]\n",
        "\n",
        "        # Go forward, remembering only some activations and no pre-activations.\n",
        "\n",
        "        # Values after activation function (including inputs to the first layer),\n",
        "        # shapes (B, N^0), (B, N^1), ..., (B, N^last);\n",
        "        # layers that are not checkpointed get None instead.\n",
        "        gs: list[FloatNDArray | None] = [x]\n",
        "\n",
        "        # TODO forward pass.\n",
        "        g = x\n",
        "\n",
        "        # Now go backward from the final cost applying backpropagation.\n",
        "        grad_g = self.cost_derivative(g, y)  # shape initially (B, N^last), then layer by layer.\n",
        "\n",
        "        # TODO backward pass.\n",
        "        # for each interval between consecutive checkpoints:\n",
        "            # Forward pass to restore intermediate activations.\n",
        "            # for all layers in interval:\n",
        "            # ...\n",
        "\n",
        "            # Now g and grad_g have shape (B, N^{end_i}).\n",
        "            assert g.shape == grad_g.shape == (B, self.sizes[end_i])\n",
        "\n",
        "            # Backward pass between the checkpoints.\n",
        "            # for all layers in the interval, backwards:\n",
        "            #   # Compute grad_w, grad_b.\n",
        "            #   # Yield them as we go, instead of returning a list.\n",
        "            #   # https://docs.python.org/3/tutorial/classes.html#generators\n",
        "            #   yield (grad_w, grad_b)\n",
        "\n",
        "            # Now grad_g has shape (B, N^{start_i}).\n",
        "            assert grad_g.shape == (B, self.sizes[start_i])\n",
        "\n",
        "    @override\n",
        "    def learning_step(self, x_mini_batch: FloatNDArray, y_mini_batch: FloatNDArray, learning_rate: float) -> None:\n",
        "        '''\n",
        "        Update network parameters with a single mini-batch step of backpropagation and gradient descent.\n",
        "\n",
        "        Args:\n",
        "        - x_mini_batch: shape (B, N^0) where B is mini_batch_size.\n",
        "        - y_mini_batch: shape (B, N^last).\n",
        "        - learning_rate.\n",
        "        '''\n",
        "        indices = range(len(self.sizes) - 1, 0, -1)  # From `last` down to 1 inclusive.\n",
        "        for i, (grad_w, grad_b) in zip(indices, self.backprop_generator(x_mini_batch, y_mini_batch), strict=True):\n",
        "            self.weights[i - 1] = self.weights[i - 1] - learning_rate * grad_w\n",
        "            self.biases[i - 1] = self.biases[i - 1] - learning_rate * grad_b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5iezw0nfgve"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Compare on the same seed, results should be exactly the same.\n",
        "\n",
        "np.random.seed(42)\n",
        "network = NetworkWithCheckpoints([784, 20, 15, 13, 10], checkpoints=[2])\n",
        "network.SGD(\n",
        "    (x_train, y_train),\n",
        "    test_data=(x_test, y_test),\n",
        "    epochs=1,\n",
        "    mini_batch_size=100,\n",
        "    learning_rate=50.\n",
        ")\n",
        "\n",
        "np.random.seed(42)\n",
        "network = NetworkVectorized([784, 20, 15, 13, 10])\n",
        "network.SGD(\n",
        "    (x_train, y_train),\n",
        "    test_data=(x_test, y_test),\n",
        "    epochs=1,\n",
        "    mini_batch_size=100,\n",
        "    learning_rate=50.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Memory usage can be checked by installing `memory_profiler` and using `%%memit` (instead of `%%time`),\n",
        " but it will be very hard to see a difference from checkpointing for an MLP."
      ],
      "metadata": {
        "id": "p2-bXJfTJOhD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9w3OzXKe42N"
      },
      "source": [
        "# JAX Playground (Optional)\n",
        "JAX is a framework that allows the creation of neural networks with numpy-like syntax.  \n",
        "In this course, we will use Pytorch instead of JAX, but for this lab scenario, JAX can help us test our gradient computation implementation.  \n",
        "Let's give it a try  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUd116BMe42O"
      },
      "outputs": [],
      "source": [
        "!pip3 install jax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaZVxk4ze42O"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from textwrap import dedent\n",
        "\n",
        "\n",
        "def jax_sigmoid(z: jax.Array) -> jax.Array:\n",
        "    return 1.0 / (1.0 + jnp.exp(-z))\n",
        "\n",
        "\n",
        "def jax_sigmoid_prime(z: jax.Array) -> jax.Array:\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "\n",
        "# Define a jax function.\n",
        "# We emphasize that this is a function, not a jax procedure,\n",
        "# and in fact there are more requirements for writing good jax code,\n",
        "# but this is just an example.\n",
        "# (see https://jax.readthedocs.io/en/latest/tutorials.html)\n",
        "def jax_forward(x: jax.Array, w: jax.Array, b: jax.Array) -> jax.Array:\n",
        "    # f = TODO\n",
        "    g = jax_sigmoid(f)\n",
        "    loss = g.sum()  # Just a dummy loss for simplicity.\n",
        "    return loss, g\n",
        "\n",
        "\n",
        "# This will calculate gradient for first, second, and third argument.\n",
        "# has_aux tells that in addition to loss our function returns something more.\n",
        "auto_backward = jax.value_and_grad(fun=jax_forward, argnums=[0, 1, 2], has_aux=True)\n",
        "\n",
        "\n",
        "def manual_backward(x: jax.Array, w: jax.Array, b: jax.Array) -> jax.Array:\n",
        "    # f = TODO\n",
        "    grad_g = jnp.ones_like(f)  # Grad of the dummy loss over activations g.\n",
        "    # grad_f = TODO\n",
        "    # grad_b = TODO\n",
        "    # grad_w = TODO\n",
        "    # grad_x = TODO\n",
        "    return grad_x, grad_w, grad_b\n",
        "\n",
        "\n",
        "def example():\n",
        "    B, N0, N1 = 3, 5, 7\n",
        "\n",
        "    key = jax.random.key(42)\n",
        "    key, subkey = jax.random.split(key)\n",
        "    w = jax.random.normal(subkey, (N1, N0))\n",
        "    key, subkey = jax.random.split(key)\n",
        "    b = jax.random.normal(subkey, (N1,))\n",
        "    x = jnp.arange(N0, dtype=w.dtype).reshape(1, N0) * jnp.ones((B, N0))\n",
        "\n",
        "    (loss, res), (jax_dx, jax_dw, jax_db) = auto_backward(x, w, b)\n",
        "    dx, dw, db = manual_backward(x, w, b)\n",
        "\n",
        "    print(dedent(f'''\n",
        "        diff dx = {jnp.mean(jnp.abs(jax_dx - dx))}\n",
        "        diff dw = {jnp.mean(jnp.abs(jax_dw - dw))}\n",
        "        diff db = {jnp.mean(jnp.abs(jax_db - db))}\n",
        "        dtype={dx.dtype} (eps={np.finfo(dx.dtype).eps})\n",
        "    ''').strip())\n",
        "\n",
        "\n",
        "example()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}