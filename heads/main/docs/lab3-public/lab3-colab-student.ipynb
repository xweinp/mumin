{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yuaOAbbRWMg"
      },
      "source": [
        "# Robot control Lab 3 - OpenCV poses and stereo\n",
        "\n",
        "Today's lab will focus on extracting 3D information from images. During creation of this scenario, Google Colab runs OpenCV version [4.8.0](https://docs.opencv.org/4.8.0/index.html). If you have a different version, you can change the documentation version to match. If you want more materials or a different approach to what will be presented in this scenario, [these materials](https://docs.opencv.org/4.6.0/d9/db7/tutorial_py_table_of_contents_calib3d.html) cover similar topics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jANnkm5rdLkf"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUeNgRCZR0UB"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "print(f\"OpenCV version is: {cv2.__version__}\")\n",
        "\n",
        "# In Colab we need to use:\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4E9OalZcCow"
      },
      "source": [
        "## Calibration parameters and undistortion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv-HGkdZSQBG"
      },
      "source": [
        "Today we will be working with a camera with given calibration parameters. This means someone else (maybe the manufacturer) performed the calibration procedure and supplied us with the results. As you probably remember from the lecture, the camera matrix, or calibration matrix looks like this:\n",
        "\n",
        "\\begin{align}\n",
        "\\left[\\begin{array}{ccc}\n",
        "f_{x} & 0 & c_{x}\\\\\n",
        "0 & f_{y} & c_{y}\\\\\n",
        "0 & 0 & 1\n",
        "\\end{array}\\right]\n",
        "\\end{align}\n",
        "\n",
        "You should also be familiar with radial and tangential distortions that can be represented with a vector of 5 numbers:\n",
        "\n",
        "\\begin{align}\n",
        "\\left(\\begin{array}{ccccc}\n",
        "k_{1} & k_{2} & p_{1} & p_{2} & k_{3}\\end{array}\\right)\n",
        "\\end{align}\n",
        "\n",
        "More on that [here](https://docs.opencv.org/4.1.2/dc/dbb/tutorial_py_calibration.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQZQJcTXs3-7"
      },
      "outputs": [],
      "source": [
        "camera_matrix = np.array([[528.86 ,   0.    , 641.865],\n",
        "                          [  0.   , 528.755 , 360.867],\n",
        "                          [  0.   ,   0.    ,   1.   ]])\n",
        "\n",
        "dist_coeffs = np.array([-0.0420881, 0.0110919, -0.00090298, -0.00013151, -0.00534522])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx27Io7mvtn-"
      },
      "source": [
        "Today's provided images are distorted. We have calibration parameters, so this should not be a problem.\n",
        "\n",
        "To **undistort** the images we need undistortion maps. Note that if we compute undistortion maps for a camera using one image, we can use the same maps to undistort other images taken with the same camera. So we need to find the maps only once.\n",
        "\n",
        "We find the maps in two steps. First we derive a new camera matrix using [getOptimalNewCameraMatrix](https://docs.opencv.org/4.1.2/d9/d0c/group__calib3d.html#ga7a6c4e032c97f03ba747966e6ad862b1). Then, we use the obtained matrix to calculate undistortion maps. Check the code below to see how this can be done. Try different values of alpha, like 0 and 1, and see how the results change. Choose the value you deem the best.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeMjfhi-wCkn"
      },
      "outputs": [],
      "source": [
        "img0 = cv2.imread(\"0.png\")\n",
        "# OpenCV order\n",
        "size = (img0.shape[1], img0.shape[0])\n",
        "\n",
        "# Calculate undistorted camera matrix\n",
        "alpha = 1 # TODO: try 0 and 1\n",
        "rect_camera_matrix = cv2.getOptimalNewCameraMatrix(camera_matrix, dist_coeffs, size, alpha)[0]\n",
        "\n",
        "# Calculate undistortion maps\n",
        "map1, map2 = cv2.initUndistortRectifyMap(camera_matrix, dist_coeffs, np.eye(3), rect_camera_matrix, size, cv2.CV_32FC1)\n",
        "\n",
        "# Use maps to undistort an image\n",
        "rect_img0 = cv2.remap(img0, map1, map2, cv2.INTER_LINEAR)\n",
        "\n",
        "# Show original and undistorted side by side\n",
        "cv2_imshow(cv2.hconcat([img0, rect_img0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZX1-P1_AlKt"
      },
      "source": [
        "## Single image, single marker pose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9qEhEYUFUIz"
      },
      "source": [
        "### Quick recap, OpenCV names:\n",
        "\n",
        "Pose = position + orientation\n",
        "\n",
        "Position is a 3D translation vector:\n",
        "$\\left[\\begin{array}{c}d_{x}\\\\d_{y}\\\\d_{z}\\end{array}\\right]$, OpenCV will call this `tvec`.\n",
        "\n",
        "Orientation can be expressed in many ways: rotation matrix, rotation vector, quaternion, euler angles (e.g. roll, pitch, yaw).\n",
        "\n",
        "OpenCV most often works with rotation vector\n",
        "$\\left[\\begin{array}{c}r_{x}\\\\r_{y}\\\\r_{z}\\end{array}\\right]$,\n",
        "and calls it `rvec`.\n",
        "\n",
        "OpenCV also provides [Rodrigues](https://docs.opencv.org/4.1.2/d9/d0c/group__calib3d.html#ga61585db663d9da06b68e70cfbf6a1eac) to convert `rvec` to and from the rotation matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUQdYw-jHodA"
      },
      "source": [
        "### Detecting the pose\n",
        "\n",
        "As you remember from the lecture, using the pinhole camera model we can map a point in 3D world coordinates to a point on the image plane. The other direction is not so easy â€” a point on the image plane gives us a line (light ray) in 3D on which this point in the real world lies. Now, we can use 4 corners of a marker, because we know their geometry: they lie on a single plane and form a square with a known side length.\n",
        "\n",
        "The process looks like this: we take corners on the image and imagine 4 rays, then we fit a square in 3D so that each corner lies on a ray and the side lengths match. This image can help you visualize the process:\n",
        "\n",
        "![Projecting points into 3D](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQsuPYWNVKA1Lo9U07uDM7vo58Hf3iAxMG35A&s)\\\n",
        "[Image source](https://ballentain.tistory.com/40)\n",
        "\n",
        "We will use a function `my_estimatePoseSingleMarkers` that solves that by utilizing `solvePnP` function from OpenCV. It takes corners, marker size, intrinsic matrix and an array containing parameters of distortions.\n",
        "\n",
        "Let's see how to use it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33N48DDfFEd9"
      },
      "outputs": [],
      "source": [
        "def my_estimatePoseSingleMarkers(corners, marker_size, mtx, distortion):\n",
        "    '''\n",
        "    This will estimate the rvec and tvec for each of the marker corners detected by:\n",
        "       corners, ids, rejectedImgPoints = detector.detectMarkers(image)\n",
        "    corners - is an array of detected corners for each detected marker in the image\n",
        "    marker_size - is the size of the detected markers\n",
        "    mtx - is the camera matrix\n",
        "    distortion - is the camera distortion matrix\n",
        "    RETURN list of rvecs, tvecs, and trash (so that it corresponds to the old estimatePoseSingleMarkers())\n",
        "\n",
        "    Stolen from StackOverflow\n",
        "    '''\n",
        "    marker_points = np.array([[-marker_size / 2, marker_size / 2, 0],\n",
        "                              [marker_size / 2, marker_size / 2, 0],\n",
        "                              [marker_size / 2, -marker_size / 2, 0],\n",
        "                              [-marker_size / 2, -marker_size / 2, 0]], dtype=np.float32)\n",
        "    trash = []\n",
        "    rvecs = []\n",
        "    tvecs = []\n",
        "    for c in corners:\n",
        "        not_important, R, t = cv2.solvePnP(marker_points, c, mtx, distortion, False, cv2.SOLVEPNP_IPPE_SQUARE)\n",
        "        rvecs.append(R)\n",
        "        tvecs.append(t)\n",
        "        trash.append(not_important)\n",
        "    return rvecs, tvecs, trash\n",
        "\n",
        "# Aruco detector parameters\n",
        "dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_APRILTAG_16h5)\n",
        "detectorParams = cv2.aruco.DetectorParameters()\n",
        "detectorParams.cornerRefinementMethod = cv2.aruco.CORNER_REFINE_CONTOUR\n",
        "detector = cv2.aruco.ArucoDetector(dictionary, detectorParams)\n",
        "\n",
        "# Note that unit is not specified, we just need to stick to one (here meters)\n",
        "MARKER_SIDE = 0.168\n",
        "\n",
        "img1 = cv2.imread(\"1.png\")\n",
        "img1_draw = img1.copy()\n",
        "\n",
        "corners, ids, _ = detector.detectMarkers(img1)\n",
        "\n",
        "# TODO: inspect the image and draw detection\n",
        "\n",
        "cv2.aruco.drawDetectedMarkers(img1_draw, corners, ids)\n",
        "cv2_imshow(img1_draw)\n",
        "\n",
        "# This takes multiple corners and calculates 3D pose\n",
        "rvecs, tvecs, _ = my_estimatePoseSingleMarkers(corners, MARKER_SIDE, camera_matrix, dist_coeffs)\n",
        "\n",
        "# TODO: inspect type and shape of rvecs and tvecs\n",
        "print(rvecs[0].shape, tvecs[0].shape)\n",
        "\n",
        "# We can draw a pose using OpenCV\n",
        "img1_draw = img1.copy()\n",
        "cv2.drawFrameAxes(img1_draw, camera_matrix, dist_coeffs, rvecs[0], tvecs[0], 0.1)\n",
        "cv2_imshow(img1_draw)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3irzuAr0VWkY"
      },
      "source": [
        "### Detections and undistortion\n",
        "\n",
        "Notice that we did not undistort the image. Marker detection often runs just fine on distorted images, but we had to pass not only the camera matrix, but also distortion coeffs to every function that calculated things in 3D. This way we didn't have to explicitly undistort the whole image, which can be slow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTjghuoqWhVg"
      },
      "source": [
        "### Your turn, but undistort first\n",
        "\n",
        "Now try to merge knowledge about undistortion and marker's pose detection and load image 2.png, undistort it, and then perform markers detection and pose calculation.\n",
        "\n",
        "**Do not recalculate** things we already have: undistortion maps, undistorted camera matrix, marker detector parameters, marker size.\n",
        "\n",
        "Note: remember that undistortion changes the camera matrix and makes distortion coefficients zero, so you should use `rect_camera_matrix` after undistortion and you can just pass `0` as distCoeffs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zQsiu0nXepL"
      },
      "outputs": [],
      "source": [
        "img2 = cv2.imread(\"2.png\")\n",
        "\n",
        "rect_img2 = cv2.remap(img2, map1, map2, cv2.INTER_LINEAR)\n",
        "img2_draw = rect_img2.copy()\n",
        "\n",
        "corners, ids, _ = detector.detectMarkers(rect_img2)\n",
        "\n",
        "cv2.aruco.drawDetectedMarkers(img2_draw, corners, ids)\n",
        "cv2_imshow(img2_draw)\n",
        "\n",
        "# This takes multiple corners and calculates 3D pose\n",
        "rvecs, tvecs, _ = my_estimatePoseSingleMarkers(corners, MARKER_SIDE, camera_matrix, np.zeros_like(dist_coeffs))\n",
        "\n",
        "# TODO: inspect type and shape of rvecs and tvecs\n",
        "print(rvecs[0].shape, tvecs[0].shape)\n",
        "\n",
        "# We can draw a pose using OpenCV\n",
        "img2_draw = rect_img2.copy()\n",
        "cv2.drawFrameAxes(img2_draw, camera_matrix, np.zeros_like(dist_coeffs), rvecs[0], tvecs[0], 0.1)\n",
        "cv2_imshow(img2_draw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvSn4JnVaiuq"
      },
      "source": [
        "### Drawing\n",
        "\n",
        "Just as in the last scenario we will draw 3D poses ourselves, but now in 3D!\n",
        "\n",
        "We will use [projectPoints](https://docs.opencv.org/4.1.2/d9/d0c/group__calib3d.html#ga1019495a2c8d1743ed5cc23fa0daff8c) function, which takes object pose as `rvec` and `tvec` and an array of object points. Object points are just additional translations in object's frame of reference. For example, if we have object point `[42, 0, 0]` and will call `projectPoints` with some `rvec` and `tvec`, then the function will chain transformations from the camera frame of reference (first by `tvec`, then `rvec`), then translate in the object's frame of reference by 42 in the X direction and finally project the resulting point to the image plane coordinates. For convenience the function can take multiple object points at once. Inspect the demo below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhxnGzWY1OHE"
      },
      "outputs": [],
      "source": [
        "WHITE = (255, 255, 255)\n",
        "BLACK = (0, 0, 0)\n",
        "GREEN = (0, 255, 0)\n",
        "DARK_GREEN = (0, 127, 0)\n",
        "RED = (0, 0, 255)\n",
        "BLUE = (255, 0, 0)\n",
        "VIOLET = (255, 0, 255)\n",
        "CYAN = (255, 255, 0)\n",
        "YELLOW = (0, 255, 255)\n",
        "ORANGE = (0, 100, 255)\n",
        "\n",
        "\n",
        "objpts = np.array([[0, 0, 0], [-0.5, 0, 0], [0.25, 0.25, 0]]) * MARKER_SIDE\n",
        "\n",
        "imgpts = np.rint(cv2.projectPoints(objpts, rvecs[0], tvecs[0], rect_camera_matrix, 0,)[0]).astype(int)\n",
        "imgpts = imgpts.reshape((-1, 2))\n",
        "\n",
        "rect_img2_draw = rect_img2.copy()\n",
        "\n",
        "cv2.circle(rect_img2_draw, (imgpts[0][0], imgpts[0][1]), 5, GREEN, -1);\n",
        "cv2.circle(rect_img2_draw, (imgpts[1][0], imgpts[1][1]), 5, VIOLET, -1);\n",
        "cv2.circle(rect_img2_draw, (imgpts[2][0], imgpts[2][1]), 5, ORANGE, -1);\n",
        "\n",
        "cv2_imshow(rect_img2_draw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-X_zOoi5qly"
      },
      "source": [
        "### Your turn! Drawing cubes\n",
        "\n",
        "On the `rect_img2` try to calculate and draw points in a cube pattern, like so:\n",
        "\n",
        "![cubes](https://github.com/nomagiclab/lab6_assets/raw/master/cubes.png)\n",
        "\n",
        "You will definitely need an array of object points that designate 8 cube corners, `rvecs` and `tvecs` (use calculated ones), `projectPoints` function and some drawing functions.\n",
        "\n",
        "You might be interested in `drawContours` function (search the docs!), but note that it takes list of contours and a contour is a list of points. This function lets you fill the contour, like the blue square in the example above or draw only the outline, like the red square in the example above.\n",
        "\n",
        "Remember to pass coordinates of `int` type to OpenCV drawing functions. Also, some of them, like `line`, expect coordinates as tuples, not lists or arrays (`drawContours` does not care)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rvecs[0].shape, len(corners))"
      ],
      "metadata": {
        "id": "Ve5nWs3a5y9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzyUBx6OcfU7"
      },
      "outputs": [],
      "source": [
        "\n",
        "im_draw = rect_img2.copy()\n",
        "contours = [c.reshape((-1, 1, 2)).astype(int) for c in corners]\n",
        "cv2.drawContours(im_draw, contours, -1, BLUE, -1)\n",
        "\n",
        "\n",
        "cube_corners = np.array([\n",
        "    (-MARKER_SIDE/2, MARKER_SIDE/2, MARKER_SIDE),\n",
        "    (MARKER_SIDE/2, MARKER_SIDE/2, MARKER_SIDE),\n",
        "    (MARKER_SIDE/2, -MARKER_SIDE/2, MARKER_SIDE),\n",
        "    (-MARKER_SIDE/2, -MARKER_SIDE/2, MARKER_SIDE),\n",
        "])\n",
        "imgpts = [\n",
        "    np.rint(\n",
        "        cv2.projectPoints(\n",
        "            cube_corners,\n",
        "            rvec, tvec,\n",
        "            rect_camera_matrix, 0,)[0]\n",
        "    ).astype(int).reshape(-1, 2)\n",
        "    for rvec, tvec in zip(rvecs, tvecs)\n",
        "]\n",
        "cv2.drawContours(im_draw, imgpts, -1, (0,255,0), 3)\n",
        "\n",
        "for ipts, ctrs in zip(imgpts, contours):\n",
        "  for p1, p2 in zip(ipts, ctrs):\n",
        "    cv2.line(im_draw, p1, p2.reshape(2, ), RED, 3)\n",
        "\n",
        "\n",
        "cv2_imshow(im_draw)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "d4E9OalZcCow"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rc-teaching-materials",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}